{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from feature_engine.selection import DropCorrelatedFeatures\n",
    "# possible change to pytorch  \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path):\n",
    "    df= pd.read_csv(path)\n",
    "    return pd.DataFrame(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropDuplicatedRowAndColumn(train, test):\n",
    "    \n",
    "    train = train.drop_duplicates()\n",
    "    drop = train.columns.duplicated()\n",
    "    return [train.loc[:,~drop], test.loc[:,~drop[:len(drop)-1]]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quasiConstantRemoval(train, threshold, test):\n",
    "    constant_filter = VarianceThreshold(threshold= threshold)\n",
    "    constant_filter.fit(train)\n",
    "    return [pd.DataFrame(constant_filter.transform(train)),pd.DataFrame(constant_filter.transform(test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropCorrelatedFeatures(train,test):\n",
    "    drop_correlated = DropCorrelatedFeatures(\n",
    "    variables=None, method='pearson', threshold=0.9)\n",
    "    drop_correlated.fit(train)\n",
    "    return [pd.DataFrame(drop_correlated.transform(train)),pd.DataFrame(drop_correlated.transform(test))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropColumnWithName(data, name):\n",
    "    column_to_drop = [c for c in data if c.startswith(name)]\n",
    "    data = data.drop(columns = column_to_drop )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(data):\n",
    "    scaler = StandardScaler()\n",
    "    return pd.DataFrame(scaler.fit_transform(data))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    df_train = loadData(\"train.csv\")\n",
    "    df_test = loadData(\"test.csv\")\n",
    "    \n",
    "    # dropping \"ID\" from both training and test data\n",
    "    ID_train = df_train[\"ID\"].copy()\n",
    "    ID_test = df_test[\"ID\"].copy()\n",
    "    df_train = df_train.drop(columns = \"ID\")\n",
    "    df_test = df_test.drop(columns = \"ID\")\n",
    "    \n",
    "\n",
    "    dup = dropDuplicatedRowAndColumn(df_train, df_test)\n",
    "    df_train = dup[0]\n",
    "    df_test = dup[1]\n",
    "   \n",
    "    y_train = df_train['TARGET'].copy()\n",
    "    df_train_x = df_train.drop(columns=\"TARGET\")\n",
    "    \n",
    "    quasi_res = quasiConstantRemoval(df_train_x, 0.01, df_test)\n",
    "    df_train_x = quasi_res[0]\n",
    "    df_test = quasi_res[1]\n",
    "    \n",
    "    correlated  = dropCorrelatedFeatures(df_train_x,df_test)\n",
    "    df_train_x = correlated[0]\n",
    "    df_test = correlated[1]\n",
    "   \n",
    "\n",
    "    df_train_x = normalise(df_train_x)\n",
    "    df_test = normalise(df_test)\n",
    "    \n",
    "    output_dim = findBestOutputSize(df_train_x, y_train)\n",
    "    \n",
    "    model = build_model(df_train_x.shape[1], output_dim)\n",
    "    model.fit(df_train_x, y_train)\n",
    "    prediction = model.predict_proba(df_test)[:,0]\n",
    "    \n",
    "    return pd.DataFrame({'ID':ID_test, \"Target\": prediction})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2479\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2399\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2507\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2465\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2498\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2528\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2433\n",
      "2003/2003 [==============================] - 5s 2ms/step - loss: 0.2468\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2482A: 0\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2456\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2605\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2557\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2572\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2510\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2531\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2612\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2580\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2494\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2532\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2566\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2590\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2612\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2580\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2630\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2617\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2628\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2679\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2580\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2610\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2658\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2693\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2622\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2715\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2622\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2633\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2664\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2632\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2690\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2603\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2598\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2704\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2643\n",
      "2003/2003 [==============================] - 2s 999us/step - loss: 0.2696\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2673\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2621\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2667\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2685\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2674\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2719\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2684\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2715\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2713\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2749\n",
      "2003/2003 [==============================] - 2s 996us/step - loss: 0.2753\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2737\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2733\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2693\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2794\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2718\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2736\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2747\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2679\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2733\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2756\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2781\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2780\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2734\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2783\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2742\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2795\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2748\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2773\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2759\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2772\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2785\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2781\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2740\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2803\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2836\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2719\n",
      "2003/2003 [==============================] - 2s 1000us/step - loss: 0.2792\n",
      "2003/2003 [==============================] - 2s 989us/step - loss: 0.2831\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2877\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2759\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2830\n",
      "2003/2003 [==============================] - 2s 991us/step - loss: 0.2789\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2794\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2796\n",
      "2003/2003 [==============================] - 2s 999us/step - loss: 0.2754\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2736\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2834\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2883\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2839\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2763\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2842\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2837\n",
      "2003/2003 [==============================] - 2s 998us/step - loss: 0.2805\n",
      "2003/2003 [==============================] - 2s 1ms/step - loss: 0.2840\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2804\n",
      "2003/2003 [==============================] - 3s 1ms/step - loss: 0.2842A: 0\n",
      "[0.15231068730354308, 0.1528145268559456, 0.15518811643123626, 0.15828638821840285, 0.16213497668504714, 0.1605364739894867, 0.16455858200788498, 0.16331611275672914, 0.16333200931549072, 0.1662281036376953]\n",
      "0.15231068730354308 0\n",
      "2226/2226 [==============================] - 3s 1ms/step - loss: 0.2437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
      "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ID    Target\n",
      "0           2  0.032010\n",
      "1           5  0.064854\n",
      "2           6  0.022014\n",
      "3           7  0.061794\n",
      "4           9  0.018291\n",
      "...       ...       ...\n",
      "75813  151831  0.097336\n",
      "75814  151832  0.016857\n",
      "75815  151833  0.019888\n",
      "75816  151834  0.026842\n",
      "75817  151837  0.016411\n",
      "\n",
      "[75818 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "result = main()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 fold and the measure is loss\n",
    "def build_model(input_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(output_dim,input_dim = input_dim, kernel_initializer='uniform', activation = 'tanh'))\n",
    "    model.add(Dense(1,input_dim = output_dim, kernel_initializer='uniform', activation = 'sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross fold to get the accuracy of the model with different size \n",
    "def findBestOutputSize(data, y):\n",
    "    data = data.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "    scores = [0] * 10\n",
    "    alpha_list = [i+1 for i in range(10)]\n",
    "    kfold = KFold(n_splits=10)\n",
    "    for alpha in alpha_list:\n",
    "        for train, test in kfold.split(data, y):\n",
    "            output_dim = getNumberOfNeurons(data, alpha)\n",
    "            model = build_model(data[train].shape[1],output_dim)\n",
    "            model.fit(data[train], y[train])\n",
    "#             data[test]\n",
    "            scores[alpha-1]+=model.evaluate(data[test], y[test], verbose=0)\n",
    "        scores[alpha-1] = scores[alpha-1]/10\n",
    "    print(scores)\n",
    "    print(min(scores),scores.index(min(scores)))\n",
    "    alpha = scores.index(min(scores)) +1\n",
    "    return getNumberOfNeurons(data, alpha)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-8db4816a0d6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumberOfNeurons(data, alpha ):\n",
    "    return (data.shape[0]/(alpha*(data.shape[1]+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
